# Event Announcement Template

## Headline

Join the AI Reading Club: Unpacking the Evolution of LLMs

## Description

Are you tired of just calling AI APIs and want to understand how they work under the hood? We use concepts like tokens, self-attention, LoRA, and top-p sampling every day, but how well do we understand the intuition and trade-offs behind them?

I am launching an AI Reading Club to read, deconstruct, and discuss 13 research papers that shaped modern Generative AI. We will trace the evolution of large language models from subword tokenization and the original Transformer, through interpretability, efficiency breakthroughs like FlashAttention, and fine-tuning and instruction tuning techniques that power today's assistants.

We will not just skim abstracts. We will focus on concepts, architectural trade-offs, and historical context.

## What We Will Cover

- Architecture: tokenization and the Transformer
- Interpretability: what attention and feed-forward layers learn
- Generation and efficiency: decoding, quantization, and FlashAttention
- Alignment: instruction tuning, LoRA/QLoRA, and why "less is more" (LIMA)

## Logistics

- Cadence: Biweekly (one paper every two weeks)
- Schedule: [Day] at [Time] [Time zone]
- Format: 10-15 minute summary by a volunteer, then 45 minutes discussion
- First session: Neural Machine Translation of Rare Words with Subword Units (BPE)
- Join link: [Discord / Zoom / Meetup]

